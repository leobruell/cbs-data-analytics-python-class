{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Deep Learning</h1>\n",
    "<li>Rough idea: mimic the human brain (not that we know how that works!)\n",
    "<li>Neural networks: learn through examples with no procedural learning algorithm\n",
    "<li>As wikipedia says: <span style=\"color:blue\">vaguely</span> inspired by animal brains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Structure of neural networks</h2>\n",
    "<li>A neural network is a directed often acyclic graph\n",
    "<li><span style=\"color:red\">Neurons</span>: nodes\n",
    "<li><span style=\"color:red\">Synapses</span>: connections (edges) between nodes that contain weights\n",
    "<li>a neuron calculates a weighted sum of its input nodes and then decides whether to fire or not (should it react to the input or not)\n",
    "<li><span style=\"color:red\">Activation functions</span>: the function that makes the activation decision\n",
    "<li><span style=\"color:red\">Layers</span>: aggregations of neurons that use the same transformation function (different layers can use different functions)\n",
    "<li><span style=\"color:red\">Input layer</span>: feature values from example cases enter the network here (think of it as the sensory organ of the network)\n",
    "<li><span style=\"color:red\">Output layer</span>: the result (action) layer (classes, continuous values)\n",
    "<li><span style=\"color:red\">Hidden layers</span>: all the layers between the input and output layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"neuron.png\",height=500,width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Example of a network</h2>\n",
    "<small>from: <i>https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg</i></small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Rough procedure: \"for dummies\"!</h2>\n",
    "<li>INITIALIZE: randomly assign weights to each connection\n",
    "<li>RUN:\n",
    "<ol>\n",
    "<li>give an example to the network\n",
    "<li>calculate the weighted sum of inputs at each neuron\n",
    "<li>apply the activation function to that weighted sum\n",
    "<li>do this layer by layer until you get the output layer values\n",
    "<li>calculate the difference between calculated values and actual values\n",
    "<li>tweak weights in the entire network so that the calculate output gets \"marginally\" closer to the actual value\n",
    "<li>rinse and repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>A super simple example</h1>\n",
    "<li>Each input case has 3 features, each of which is either a 0 or a 1\n",
    "<li>We need to classify each input case into either a 0 or 1 (binary classification)\n",
    "<li>The actual \"real world\" rule is that <i>if feature 1 is 0, the case is classified as 0. If feature 1 is 1, the case is classified as 1</i></li>\n",
    "<li>(In other words, features 2 and 3 contain no information)</li>\n",
    "<li>We want the net to learn this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"simple_network.png\",height=500,width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define inputs and outputs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "y = np.array([[0,0,1,1]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Initialization</h3>\n",
    "<li>generate random weights for every edge in the network\n",
    "<li>we'll use np.random.random (generates random numbers between 0 and 1)\n",
    "<li>and adjust the weights so that they are between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "syn0 = 2*np.random.random((3,1)) - 1\n",
    "syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define the activation function</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Let's see what our weighted sums look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(X,syn0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Nice. We could say that anything less than 0 is a 0 and anything greater than 0 is a 1\n",
    "<li>And use these \"predicted\" 0s and 1s to compute the error\n",
    "<li>And use these errors to adjust weights\n",
    "<li>Not ideal, because:\n",
    "<ul>\n",
    "<li>learning would be binary and the model would keep switching from class 0 to class 1 \n",
    "<li>what we would like is for learning to be smooth\n",
    "<li>\"hmm, looks like a class 1 but i'll just tweak the probability that it is a class 1 rather than switch entirely to a class 1\"\n",
    "<li>\"that way, over time, I'll get more and more sure\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sigmoid function</h2>\n",
    "<small>https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg</small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function takes a number as its argument and returns a value between 0 and 1. It has the following properties:<br>\n",
    "$ sigmoid(x) = \\dfrac{1}{1+e^{-x}} $\n",
    "<li>sigmoid(0) returns 0.5</li>\n",
    "<li>the derivative (slope) of the function is higher as values approach 0 and lower as values move away from 0</li>\n",
    "<li>the derivative is easy to calculate because it is a function of the sigmoid value</li>\n",
    "$ \\dfrac{d}{dx}sigmoid(x) = sigmoid(x)*(1-sigmoid(x)) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Only if you're interested in deriving the derivative</h3>\n",
    "The quotient rule for derivates is<p>\n",
    "$ \\frac{d }{dx}\\frac{f(x)}{g(x)} = \\dfrac{(g(x)\\times\\frac{d }{dx}f(x)) -(f(x)\\times\\frac{d}{dx}g(x)))}{g(x)^2} $\n",
    "    </p>\n",
    "    Therefore: <p>\n",
    "$ \\dfrac{d}{dx}sigmoid(x) $ <br>\n",
    "    $ = \\dfrac{(1+e^{-x})*(0) - (1)(-e^{-x})}{(1+e^{-x})^2} $<br>\n",
    "    $ = \\dfrac{ - (1)(-e^{-x})}{(1+e^{-x})^2} $<br>\n",
    "    $ = \\dfrac{e^{-x}}{(1+e^{-x})^2} $<br>\n",
    "    $ = \\dfrac{1 + e^{-x} - 1}{(1+e^{-x})^2} $<br>\n",
    "    $ = \\dfrac{1 + e^{-x}}{(1+e^{-x})^2} - \\dfrac{1}{(1+e^{-x})^2} $<br>\n",
    "    $ = \\dfrac{1}{(1+e^{-x})} - \\dfrac{1}{(1+e^{-x})^2} $<br>\n",
    "    $ = \\dfrac{1}{(1+e^{-x})}\\times(1-\\dfrac{1}{(1+e^{-x})}) $ <br>\n",
    "    $ = sigmoid(x)*(1-sigmoid(x)) $\n",
    "    </p>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "print(sigmoid(5.5) - sigmoid(5.45))\n",
    "print(sigmoid(1.0) - sigmoid(0.95))\n",
    "print(sigmoid(.5) - sigmoid(.45))\n",
    "print(sigmoid(0.05) - sigmoid(0.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Now let's train the network</h2>\n",
    "<li>First we'll multiply X by the weight array\n",
    "<li>The linear combination we talked about\n",
    "<li>Then apply the sigmoid function to introduce nonlinearity and convert the total to a (0,1) range\n",
    "<li>level_0: First (input layer)\n",
    "<li>level_1: Second (hidden layer - output layer in our case)\n",
    "<li>Then compute the error (y - level_1)\n",
    "<li>Finally adjust the weights\n",
    "<ul>\n",
    "<li>The sigmoid function gives us the rate at which we want to move the weights\n",
    "<li>multiply the error by the slope (derivative) at that point\n",
    "<li>the slope at point x is d/dx sigmoid(x) = sigmoid(x)*(1-sigmoid(x))\n",
    "<li>notice that the slope is lower at the extremes and higher in the middle!\n",
    "<li>Use these deltas to adjust the weights\n",
    "</ul>\n",
    "<li>Repeat with the next set of training cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x,deriv=False):\n",
    "    if deriv:\n",
    "        return sigmoid(x)*(1-sigmoid(x))\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "print(sigmoid(-1),sigmoid(-1,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x,deriv=False):\n",
    "    if deriv:\n",
    "        return sigmoid(x)*(1-sigmoid(x))\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def run_net(X,y,activation_function=sigmoid,passes=10):\n",
    "    np.random.seed(1) #seed the random numbers\n",
    "    syn0 = 2*np.random.random((3,1)) - 1    #Calculate initial weights\n",
    "    for i in range(0,passes):\n",
    "        level_0 = X  #Input to the nn \n",
    "        level_1 = activation_function(np.dot(level_0,syn0)) #New weights \n",
    "        level_1_error = y - level_1 #error (note: y is 1/0; level_1 is in the range (0,1))\n",
    "        #Get the derivative of the sigmoid (the change) and multiply by the error\n",
    "        level_1_delta = level_1_error * activation_function(level_1,True)\n",
    "        syn0 += np.dot(level_0.T,level_1_delta) #Update the weights (level_0 * deltas)\n",
    "    return syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weights = run_net(X,y,sigmoid,10)\n",
    "final_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.array([[1,1,1],[0,1,1],[1,0,0],[0,0,1]])\n",
    "sigmoid(np.dot(test_X,final_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Walkthrough</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_0 = X #Input layer\n",
    "syn0 = 2*np.random.random((3,1)) - 1 #initial weights\n",
    "level_1 = sigmoid(np.dot(level_0,syn0)) #predicted y's\n",
    "level_1_error = y - level_1 #error\n",
    "level_1_delta = level_1_error * sigmoid(level_1,True) #change factor\n",
    "syn0 += np.dot(level_0.T,level_1_delta) #new weights\n",
    "print(\"level_0\")\n",
    "print(level_0,\"\\n\")\n",
    "print(\"syn0\")\n",
    "print(syn0,\"\\n\")\n",
    "print(\"level_1\")\n",
    "print(level_1,\"\\n\")\n",
    "print(\"y\")\n",
    "print(y,\"\\n\")\n",
    "print(\"level_1_error\")\n",
    "print(level_1_error,\"\\n\")\n",
    "print(\"deriv\")\n",
    "print(sigmoid(level_1,True),\"\\n\")\n",
    "print(\"level_1_delta\")\n",
    "print(level_1_delta,\"\\n\")\n",
    "print(\"new weights\")\n",
    "print(syn0,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Great! Let's try a different input</h2>\n",
    "<li>If any two  or three are 1, then the class is 1\n",
    "<li>Otherwise the class is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1],\n",
    "             [1,1,0],\n",
    "             [0,1,0],\n",
    "             [1,0,0],\n",
    "             [1,0,0]])\n",
    "                \n",
    "y = np.array([[0],[1],[1],[1],[1],[0],[0],[0]])\n",
    "\n",
    "final_weights = run_net(X,y,sigmoid,10000)\n",
    "test_X = np.array([[1,1,1],[0,1,1],[1,0,0],[0,0,1]])\n",
    "sigmoid(np.dot(test_X,final_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Not so good this time~</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>The pattern here is non-linear</li>\n",
    "<li>Nonlinearities can be captured by adding layers to the network</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Three layer network</h2>\n",
    "<li>Input\n",
    "<li>Hidden\n",
    "<li>Output\n",
    "<li>We need two sets of weights \n",
    "<li>Input layer has 3 nodes\n",
    "<li>Hidden layer has 4 nodes\n",
    "<li>Output layer has 1 node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"multi layer network.png\",height=500,width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Initialize</h2>\n",
    "<li>randomly assign weights at each level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "syn1 = 2*np.random.random((4,1)) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>feed forward network</h3>\n",
    "<li>Calculate node outputs at one level</li>\n",
    "<li>Propagate changes to the next level</li>\n",
    "<li>Information (input data) moves in one direction from the input layer to the output layer</li>\n",
    "<li>There are no cycles</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_0 = X\n",
    "level_1 = sigmoid(np.dot(level_0,syn0))\n",
    "level_2 = sigmoid(np.dot(level_1,syn1))\n",
    "level_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Backpropagation</h2>\n",
    "<li><b>backpropagation</b> refers to the algorithm used to adjust weights after the error term is computed</li>\n",
    "<li>The error is propagated back from the output layer to the input layer one layer at a time, adjusting weights along the way</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_2_error = y - level_2\n",
    "level_2_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_2_delta = level_2_error*sigmoid(level_2,deriv=True)\n",
    "level_2_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Next, propagate the deltas back toward the input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_1_error = level_2_delta.dot(syn1.T)\n",
    "level_1_delta = level_1_error * sigmoid(level_1,deriv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Calculate the new weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn1 += level_1.T.dot(level_2_delta)\n",
    "syn0 += level_0.T.dot(level_1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Putting it all together</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x,deriv=False):\n",
    "    if deriv:\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def run_net(X,y,activation_function=sigmoid,passes=10):\n",
    "    import time\n",
    "    np.random.seed(1)\n",
    "    syn0 = 2*np.random.random((3,4)) - 1\n",
    "    syn1 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "    for i in range(passes):\n",
    "        level_0 = X\n",
    "        level_1 = activation_function(np.dot(level_0,syn0))\n",
    "        level_2 = activation_function(np.dot(level_1,syn1))\n",
    "\n",
    "        level_2_error = y - level_2\n",
    "\n",
    "        level_2_delta = level_2_error*activation_function(level_2,deriv=True)\n",
    "\n",
    "        level_1_error = level_2_delta.dot(syn1.T)\n",
    "\n",
    "        level_1_delta = level_1_error * activation_function(level_1,deriv=True)\n",
    "\n",
    "        syn1 += level_1.T.dot(level_2_delta)\n",
    "        syn0 += level_0.T.dot(level_1_delta)\n",
    "    print(level_2)\n",
    "    return syn0,syn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn0,syn1 = run_net(X,y,activation_function=sigmoid,passes=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Applying the net to test cases</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_0 = test_X\n",
    "level_1 = sigmoid(np.dot(level_0,syn0))\n",
    "level_2 = sigmoid(np.dot(level_1,syn1))\n",
    "level_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
