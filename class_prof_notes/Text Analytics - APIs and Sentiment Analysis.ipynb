{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1947ae83",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red\">Text Analytics</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107c9794",
   "metadata": {},
   "source": [
    "<li> <span style=\"color:red\">Text Analytics</span> is the process of using a machine to extract <span style=\"color:green\">relevant</span> information from text</li>\n",
    "<li>common packages:</li>\n",
    "<ul>\n",
    "    <li>nltk: python's native natural language toolkit. <a href=\"https://www.nltk.org/data.html\">documentation</a></li>\n",
    "    <li>stanford corenlp: Java based package for text analytics with APIs in numerous languages (including python) <a href=\"https://stanfordnlp.github.io/CoreNLP/\">documentation</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a726de44",
   "metadata": {},
   "source": [
    "<li>install nltk using pip <span style=\"color:blue\">!pip install nltk --upgrade</span></li>\n",
    "<li>My version: 3.8.1</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c248c9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/leo/anaconda3/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in /home/leo/anaconda3/lib/python3.9/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in /home/leo/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /home/leo/anaconda3/lib/python3.9/site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/leo/anaconda3/lib/python3.9/site-packages (from nltk) (2022.7.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ade1a0",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">nltk data</span>\n",
    "<li>install corpora, grammar, pre-trained models, etc. (see next cell)</li>\n",
    "<li>This is a data download - you need to do this only once!</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78b5d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT: nltk.download() install all, and then close the window to continue using your notebook\n",
    "#All files will be downloaded to a folder nltk_data in your home directory\n",
    "\n",
    "#The download window will probably come up behind your browser, so look for it\n",
    "\n",
    "#You will need to close it to be able to use your notebook again\n",
    "\n",
    "#This downloads all the stuff you need so you YOU NEED TO DO THIS ONLY ONCE!!!\n",
    "\n",
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da2c447",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">ntlk resources</span> \n",
    "<p></p>\n",
    "<li><a href=\"https://www.nltk.org/\">https://www.nltk.org/</a></li>\n",
    "<li><a href=\"https://www.nltk.org/book/\">The nltk book</a></li>\n",
    "<li><a href=\"https://cheatography.com/murenei/cheat-sheets/natural-language-processing-with-python-and-nltk/\">quick reference</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c80c0e",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:40px\">Types of analysis</span>\n",
    "<br><br>\n",
    "<li>Sentiment analysis: Deciding whether a document (or concept) is positive or negative\n",
    "<li>Entity analysis: Identifying entities (Named entities, Parts of speech) and properties of these entities\n",
    "<li>Topic analysis: Identifying the major topics associated with a piece of text\n",
    "<li>Text summarization: Summarizing a document (Cliff notes version!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13baa817",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Sentiment Analysis</span>\n",
    "<p></p>\n",
    "<span style=\"color:blue;font-size:large\">Basic idea:</span>\n",
    "\n",
    "<li>Identify entities and emotions in a sentence and use these to determine if the entity is being viewed positively or negatively</li>\n",
    "\n",
    "<p></p>\n",
    "<span style=\"color:blue;font-size:large\">Easy examples</span>\n",
    "<li>I had an <b style=\"color:green\">excellent</b> souffle at the restaurant Cavity Maker</li>\n",
    "<li>Excellent is a positive word for both the souffle as well as for the restaurant</li>\n",
    "<p></p>\n",
    "<span style=\"color:blue;font-size:large\">Not so easy examples</span>\n",
    "<li>Often, looking at words alone is not enough to figure out the sentiment</li>\n",
    "<ul>\n",
    "<li><i>The Girl on the Train is an <span style=\"color:green\">excellent</span> book for a ‘stuck at home’ snow day</i></li> This one is easy since it includes an explicit positive opinion using a positive word<p>\n",
    "<li><i>The Girl on the Train is an <span style=\"color:green\">excellent</span> book for using as a liner for your cat’s litter box</i></li> Not so simple! The positive word \"excellent\" is used with a negative connotation. <p>\n",
    "<li><i>The Girl on the Train is <span style=\"color:green\">better</span> than Gone Girl</i></li> The positive word is used as a comparator. Whether the writer likes The Girl on the Train or not depends on what he or she thinks of Gone Girl\n",
    "    </ul>\n",
    "\n",
    "<p></p>\n",
    "<span style=\"color:blue;font-size:large\">Bottom line</span>\n",
    "<li>Sentiment analysis is generally a starting point in analyzing a text and is then coupled with other techniques (e.g., topic analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9955d9f3",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Basic sentiment analysis</span>\n",
    "<p></p>\n",
    "<li>Usually done using a corpus of positive and negative words</li>\n",
    "<li>Some sources compile lists of positive and negative words\n",
    "<li>Others include the polarity - the degree of positivity or negativity - of each word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd5c98",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Sources of sentiment coded words</span>\n",
    "<p></p>\n",
    "<ol>\n",
    "<li>Hu and Liu's sentiment analysis lexicon: words coded as either positive or negative</li>\n",
    "<ul>\n",
    "<li>http://ptrckprry.com/course/ssd/data/positive-words.txt\n",
    "<li>http://ptrckprry.com/course/ssd/data/negative-words.txt\n",
    "</ul><p>\n",
    "<li>NRC Emotion Lexicon: words coded into emotional categories (many languages)</li>\n",
    "<ul>\n",
    "<li>http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm</li>\n",
    "</ul><p>\n",
    "<li>SentiWordNet: Lists of words weighted by positive or negative sentiment. Includes guidance on how to use the words</li>\n",
    "<ul>\n",
    "<li>http://sentiwordnet.isti.cnr.it/</li>\n",
    "</ul><p>\n",
    "<li>Vadar Sentiment tool: 7800 words with positive or negative polarity</li>\n",
    "<ul>\n",
    "<li>Included with python nltk</li>\n",
    "</ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81301445",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Polarity based sentiment analysis</span>\n",
    "<p></p>\n",
    "<li><span style=\"color:blue\">VADER (Valence Aware Dictionary for Sentiment Reasoning)</span> is a model based analyzer that assigns polarity to words and also attempts to use the context in which the word is used</li>\n",
    "<li>VADER can distinguish between \"I love you\" (positive); \"I don't love you\" (negative); and \"I don't hate you\" (neutral) because it assigns a polarity to love and hate but uses the don't in a smart way</li>\n",
    "<li>In other words, VADER calculates a sentiment on an entire sentence, not just on individual words</li>\n",
    "<li>VADER also reports a compound score (-1 to +1) that reflects the net positivity or negativity of a sentence</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b701c754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in /home/leo/anaconda3/lib/python3.9/site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /home/leo/anaconda3/lib/python3.9/site-packages (from vaderSentiment) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/leo/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/leo/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/leo/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/leo/anaconda3/lib/python3.9/site-packages (from requests->vaderSentiment) (1.26.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b43c0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.323, 'pos': 0.677, 'compound': 0.6369}\n",
      "{'neg': 0.529, 'neu': 0.471, 'pos': 0.0, 'compound': -0.5216}\n",
      "{'neg': 0.0, 'neu': 0.5, 'pos': 0.5, 'compound': 0.4585}\n",
      "{'neg': 0.649, 'neu': 0.351, 'pos': 0.0, 'compound': -0.5719}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "print(analyzer.polarity_scores(\"I love you\")) #Net positive sentence\n",
    "print(analyzer.polarity_scores(\"I don't love you\")) #Negative sentence because of the \"don't\"\n",
    "print(analyzer.polarity_scores(\"I don't hate you\")) #Positive sentence because of the \"don't\"\n",
    "print(analyzer.polarity_scores(\"I hate you\")) #Net negative (but not as negative as the first one was positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f407471",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">API: Application Programming Interface</span>\n",
    "<p></p>\n",
    "<li>An API is a specification (often a defined set of functions) that allow one program to communicate with another program</li>\n",
    "<li>In analytics, APIs are often used to get data from some data resource (e.g., twitter, yelp)</li>\n",
    "<li>Most data providers require that API users obtain authentication keys. This allows the provider to monitor API usage at the user level</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c4a8b",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Yelp API: Sentiment analysis of local restaurants</span>\n",
    "<p></p>\n",
    "<li>Yelp has an API (<a href=\"https://www.yelp.com/developers/documentation/v3\">Yelp Fusion</a>)</li>\n",
    "<li>Each corpus element will consist of a tuple (document id, document text)</li>\n",
    "\n",
    "<span style=\"color:red;\">If you want to do this (but you don't need to do it now)</span>\n",
    "<li>Log into yelp (https://yelp.com). Create an account if you don't have one</li>\n",
    "<li>Go to <a href=\"https://www.yelp.com/developers/documentation/v3\">Yelp Fusion</a></li>\n",
    "<li>Click <span style=\"color:blue\">Manage API Access</span> on the top  menu bar</li>\n",
    "<li>Enter app info (leave optional stuff blank)\n",
    "<li>Copy the client id and API key to a secure place (this notebook should do the trick or use a text file!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24604df",
   "metadata": {},
   "source": [
    "<li>Get reviews of local restaurants from Yelp</li>\n",
    "<ul>\n",
    "    <li>The API URL: https://api.yelp.com/v3/businesses/search returns a list of businesses that match given criteria (location, number, type)</li>\n",
    "    <li>Each returned business has a Yelp business ID</li>\n",
    "    <li>Using this business ID, the API: https://api.yelp.com/v3/businesses/ returns data about the business</li>\n",
    "    <li>Yelp returns a \"review snippet\" consisting of the first sentences of three revies</li>\n",
    "    <li>We'll use these snippets to get the sentiment for each restaurant</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4407594",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/hardeepjohar/Documents/Courses/AppCredentials/credentials/yelp_fusion.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55054/1139397929.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#My API keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/hardeepjohar/Documents/Courses/AppCredentials/credentials/yelp_fusion.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mCLIENT_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mAPI_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/hardeepjohar/Documents/Courses/AppCredentials/credentials/yelp_fusion.txt'"
     ]
    }
   ],
   "source": [
    "#My API keys \n",
    "with open('/Users/hardeepjohar/Documents/Courses/AppCredentials/credentials/yelp_fusion.txt','r') as f:\n",
    "    CLIENT_ID = f.readline().strip()\n",
    "    API_KEY = f.readline().strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c20280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API constants, you shouldn't have to change these.\n",
    "API_HOST = 'https://api.yelp.com' #The API url header\n",
    "SEARCH_PATH = '/v3/businesses/search' #The path for an API request to find businesses\n",
    "BUSINESS_PATH = '/v3/businesses/'  # The path to get data for a single business\n",
    "\n",
    "#This function gets the list of businesses near the location\n",
    "def get_restaurants(api_key,location,number=15):\n",
    "    import requests\n",
    "    \n",
    "     #Set up the search data dictionary. This contains the search parameters\n",
    "    search_data = {\n",
    "    'term': \"restaurant\",\n",
    "    'location': location.replace(' ', '+'),\n",
    "    'limit': number\n",
    "    }\n",
    "    \n",
    "    #Create the API url\n",
    "    url = API_HOST + SEARCH_PATH\n",
    "    \n",
    "    #The API Key data object\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer %s' % api_key,\n",
    "    }\n",
    "    \n",
    "    #Get the response object and convert from JSON into a python list\n",
    "    response = requests.request('GET', url, headers=headers, params=search_data).json()\n",
    "    #print(response)\n",
    "    \n",
    "    #Extract the businesses from the response object\n",
    "    businesses = response.get('businesses')\n",
    "    \n",
    "    #Extract the business id and business name for each business into a list\n",
    "    return_data = [(business['id'],business['name']) for business in businesses]\n",
    "    return return_data\n",
    "\n",
    "#This function gets reviews for each business from yelp\n",
    "def get_business_review(api_key,business_id):\n",
    "    import requests\n",
    "    \n",
    "    #API path\n",
    "    business_path = BUSINESS_PATH + business_id+\"/reviews\"\n",
    "    \n",
    "    #API url\n",
    "    url = API_HOST + business_path\n",
    "\n",
    "    #API Key data object\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer %s' % api_key,\n",
    "    }\n",
    "\n",
    "    #Get the response and convert from json into a python object\n",
    "    response = requests.request('GET', url, headers=headers).json()\n",
    "    #print(response)\n",
    "    \n",
    "    #Yelp gives review snippets of a few reviews, concatenate these into a single string\n",
    "    review_text = ''\n",
    "    for review in response['reviews']:\n",
    "        review_text += review['text']\n",
    "    return review_text\n",
    "\n",
    "#A function that puts all this together\n",
    "def get_reviews(location,number=15):\n",
    "\n",
    "    #Get the business id and names for restaurants\n",
    "    restaurants = get_restaurants(API_KEY,location,number)\n",
    "\n",
    "    #Take care of the case where there are no restaurants\n",
    "    if not restaurants:\n",
    "        return None\n",
    "    \n",
    "    #Collect reviews by iterating through the restaurants and getting reviews\n",
    "    review_list = list()\n",
    "    for restaurant in restaurants:\n",
    "        restaurant_name = restaurant[1]\n",
    "        restaurant_id = restaurant[0]\n",
    "        review_text = get_business_review(API_KEY,restaurant_id)\n",
    "        \n",
    "        review_list.append((restaurant_name,review_text))\n",
    "    return review_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_corpus_cbs = get_reviews(\"Columbia Business School, New York, NY\")\n",
    "yelp_corpus_lbs = get_reviews(\"London Business School\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6d7202",
   "metadata": {},
   "source": [
    "<h4>A function to get the vader sentiment of a document</h4>\n",
    "<li>Get the average sentiment of the sentences in the document</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ff8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_sentiment(doc):\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    from nltk import sent_tokenize\n",
    "\n",
    "    #Tokenize into sentences (since vader works best on sentences)\n",
    "    sentences = sent_tokenize(doc)\n",
    "    \n",
    "    #Initlalize all sentiments to 0\n",
    "    pos=compound=neu=neg=0\n",
    "    for sentence in sentences:\n",
    "        #Get the sentiments\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        pos+=vs['pos']/(len(sentences))\n",
    "        compound+=vs['compound']/(len(sentences))\n",
    "        neu+=vs['neu']/(len(sentences))\n",
    "        neg+=vs['neg']/(len(sentences))\n",
    "    return pos, neg, neu, compound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18eb171",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_sentiment(yelp_corpus_cbs[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75de777",
   "metadata": {},
   "source": [
    "<h4>A function that returns the Vader sentiment of a corpus</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_comparison(corpus):\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    from nltk import sent_tokenize\n",
    "    import pandas as pd\n",
    "    headers = ['doc_id','pos','neg','neu','compound']\n",
    "    df = pd.DataFrame(columns=headers)\n",
    "    df.set_index('doc_id',inplace=True)\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    for doc in corpus:\n",
    "        doc_id = doc[0]\n",
    "        pos,neg,neu,compound = vader_sentiment(doc[1])\n",
    "        df.loc[doc_id] = [pos,neg,neu,compound]\n",
    "    return df.sort_values(by=\"compound\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eab98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_comparison(yelp_corpus_cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b8508",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_comparison(yelp_corpus_lbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31fd29a",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Twitter API: Sentiment analysis of stock tweets</span>\n",
    "<p></p>\n",
    "<li>Twitter uses a protocol known a \"Open Authorization (OAuth)\"</li>\n",
    "<li>Open Authorization is used by applications to allow a user to share information about their account with third party websites or applications</li>\n",
    "<li>Example, using facebook, google, or twitter to log into another application</li>\n",
    "<li>OAuth allows the user to login once and then collect information without sending a key</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103eba9b",
   "metadata": {},
   "source": [
    "<h2>tweepy</h2>\n",
    "<li>A python library that interfaces with the twitter API\n",
    "<li>Returns a lot of useful stuff but we'll only look at the tweet text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6716f60c",
   "metadata": {},
   "source": [
    "<h3>Twitter API set-up</h3>\n",
    "<li>See <a href=\"https://developer.twitter.com/en/docs/getting-started\">this</a></li>\n",
    "<li>And <a href=\"https://developer.twitter.com/en/products/twitter-api/academic-research\">this</a></li>\n",
    "<li>Unfortunately, twitter has become very difficult to use without a paid account so don't try this at home!</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5048efb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/hardeepjohar/Documents/Courses/AppCredentials/credentials/twitter_tokens.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10828/769765206.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#My keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/hardeepjohar/Documents/Courses/AppCredentials/credentials/twitter_tokens.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtoken_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mconsumer_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mconsumer_secret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/hardeepjohar/Documents/Courses/AppCredentials/credentials/twitter_tokens.txt'"
     ]
    }
   ],
   "source": [
    "#My keys\n",
    "with open(\"/Users/hardeepjohar/Documents/Courses/AppCredentials/credentials/twitter_tokens.txt\",'r') as token_file:\n",
    "    contents = token_file.read().split('\\n')\n",
    "    consumer_key = contents[0]\n",
    "    consumer_secret = contents[1]\n",
    "    access_token = contents[2]\n",
    "    access_token_secret = contents[3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8c5129",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'consumer_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10828/3568884130.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m def get_tweets(search_term,consumer_key=consumer_key,consumer_secret=consumer_secret,\n\u001b[0m\u001b[1;32m      2\u001b[0m                access_token=access_token,access_token_secret=access_token_secret):\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#Authentication token setup with tweepy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'consumer_key' is not defined"
     ]
    }
   ],
   "source": [
    "def get_tweets(search_term,consumer_key=consumer_key,consumer_secret=consumer_secret,\n",
    "               access_token=access_token,access_token_secret=access_token_secret):\n",
    "    import tweepy\n",
    "    \n",
    "    #Authentication token setup with tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    \n",
    "    #Authenticate with twitter\n",
    "    api = tweepy.API(auth)\n",
    "    \n",
    "    #Get the tweets. Twitter limits the number so we won't get 1000 but 100. results is a list of tweets \n",
    "    results = api.search_tweets(q = search_term, lang = \"en\", result_type = \"recent\", count = 1000)\n",
    "\n",
    "    #Write the tweets to files. \n",
    "    for i in range(len(results)):\n",
    "        fname = search_term+'.'+str(len(results)-i).zfill(3) #Example: AAPL.100 (the tweets come last first, so we'll reverse number them)\n",
    "        #I'm saving them in a folder called tweets\n",
    "        with open('tweets/'+fname,'w') as f:\n",
    "            f.write(results[i]._json['text']+'\\n')\n",
    "\n",
    "#get tweets for goog and aapl            \n",
    "get_tweets(search_term=\"GOOG\")\n",
    "get_tweets(search_term=\"AAPL\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2c6d7c",
   "metadata": {},
   "source": [
    "<h2>trend analysis on tweets</h2>\n",
    "<li>Assuming we had a large collection of tweets, we could do a sentiment trend analysis</li>\n",
    "<li>Since the tweets are numbered with more recent having higher numbers</li>\n",
    "<li><span style=\"color:blue\">analyze_sentiment_trend</span> creates a corpus for a single ticker where each document is a tweet</li>\n",
    "<li>and sentiment analyzes this corpus</li>\n",
    "<li>we can then use pandas rolling and mean functions to construct moving averages of sentiments</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc01226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_trend(ticker):\n",
    "    tweets_root = \"/Users/hardeepjohar/Documents/Courses/Fall2020/data/tweets/\"\n",
    "    file_pattern = ticker + \".*\"\n",
    "    import glob #glob gets all files in a directory that match a pattern\n",
    "    import pandas as pd\n",
    "    sorted_file_list = sorted(glob.glob(tweets_root + file_pattern))\n",
    "    \n",
    "\n",
    "    all_data = list()\n",
    "    for file in sorted_file_list:\n",
    "        with open(file,'r') as f:\n",
    "            all_data.append((str(file.split('.')[1]),f.read()))\n",
    "    df = vader_comparison(all_data)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e012b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_sentiment_df = analyze_sentiment_trend(\"AAPL\")\n",
    "aapl_sentiment_df.compound.sort_index().rolling(8).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7457fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "goog_sentiment_df = analyze_sentiment_trend(\"GOOG\")\n",
    "goog_sentiment_df.compound.sort_index().rolling(8).mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc36fb",
   "metadata": {},
   "source": [
    "<span style=\"color:red;font-size:40px\">Named entity based sentiment analysis</span>\n",
    "<p></p><li>A <span style=\"color:blue\">named entity</span> is a real world object that can be denoted with a proper name</li>\n",
    "<li>Named entities may be people (e.g., Joe Biden, Bob Dylan), geographical locations (The United States of America, Taj Mahal), organizations (Columbia University, World Bank, Oxfam)</li>\n",
    "<li>Named entities are often the subject of sentiments so identifying them can be very useful</li>\n",
    "<li>Named entities may span more then one word (e.g., Columbia University)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9597ff6f",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Finding named entities with spaCy</span>\n",
    "<li><span style=\"color:blue\">spaCy</span> is a NLP model trained on masses of data in various languages</li>\n",
    "<li>Good at named entity recognition</li>\n",
    "<li><a href=\"https://spacy.io/usage#quickstart\">Installation guide</a></li>\n",
    "<li>Install spacy and install the english language trained model en_core_web_sm (3.2.4)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2135a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2670cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47756ff0",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Using spaCy</span>\n",
    "<li>Load the language model</li>\n",
    "<li>get the named entities</li>\n",
    "<li>spacy returns each entity, its location in the text, and the entity type (ORG, PERSON, DATE, MONEY, GPE, etc.)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55ce39f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e68e0458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "late afternoon 8 22 TIME\n",
      "a summer day 26 38 DATE\n",
      "New York 42 50 GPE\n",
      "twenty dollars 58 72 MONEY\n",
      "John Smith 167 177 PERSON\n",
      "Columbia University 200 219 ORG\n",
      "John Smith 311 321 PERSON\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "It was late afternoon on a summer day in New York. I had twenty dollars in my pocket.\n",
    "I was walking along thinking of many things. \n",
    "For e.g., I walked with my friend John Smith through the campus of Columbia University. I \n",
    "thought of birds, of bees, of sealing wax. I thought of cabbages and kings. My name is John Smith.\n",
    "\"\"\"\n",
    "doc = nlp(sample_text)\n",
    "for ent in doc.ents: #ent.text contains the ne; ent.label_ contains the type of ne\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68534643",
   "metadata": {},
   "source": [
    "<h3>Sentiment associated with entities in the news</h3>\n",
    "<li>We'll get the latest stories from www.slate.com</li>\n",
    "<li>Extract entities from these stories</li>\n",
    "<li>And then calculate the sentiment associated with the entities (in the stories) as the average sentiment of the sentences that contain the entity</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9426464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get stories from slate\n",
    "def get_slate_stories():\n",
    "    #followable_links contains the links to news and politics stories\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    url=\"https://www.slate.com\"\n",
    "    page = requests.get(url)\n",
    "    bs_page = BeautifulSoup(page.content,'lxml')\n",
    "    all_links = bs_page.find_all('a')\n",
    "    \n",
    "    #Define the list of categories that we want to follow\n",
    "    categories = ['news-and-politics']\n",
    "    \n",
    "    #followable_links will contain the title and the detail of each story\n",
    "    followable_links = list()\n",
    "    for link in all_links:\n",
    "        href = link.get('href') #get the link\n",
    "        if href: #If the link exists (sometimes it doesn't!)\n",
    "            for cat in categories:\n",
    "                if cat in href: #Only stories in the category\n",
    "                    title = link.get_text().strip() #Get the story title\n",
    "                    followable_links.append((title,href)) #Append (title, link) to followable links\n",
    "    \n",
    "    \n",
    "    #Iterate through followable links extracting the text of each story\n",
    "    #story_list is a list of the stories\n",
    "    #Note that some links will not contain an article_body section, those will be ignored (that's why the try except)\n",
    "    story_list = list()\n",
    "    count=0\n",
    "    for link in followable_links:\n",
    "        try:\n",
    "            page=BeautifulSoup(requests.get(link[1]).content,'lxml')\n",
    "            text=page.find('body').find('section',class_='article__body').get_text().strip()\n",
    "            story_list.append((link[0],text))\n",
    "            count+=1\n",
    "        except:\n",
    "            continue\n",
    "    return story_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec0873c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "slate_corpus = get_slate_stories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3946e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function that gets named entities in a corpus\n",
    "def get_ne_from_corpus(corpus,types=[]): #If types is empty then all types, otherwise just the ones in the list\n",
    "    \n",
    "    #Set up spacy's ne analyzer\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    #Accumulate all the text in a single string (since we're looking across all articles)\n",
    "    all_text = \"\"\n",
    "    for doc in corpus:\n",
    "        all_text += doc[1]\n",
    "    doc = nlp(all_text)\n",
    "    text_ents = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in types or not types:\n",
    "            text_ents.add((ent.text,ent.label_))\n",
    "    return text_ents   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "860418f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Advertisement\\n\\n\\n\\n', 'PERSON'),\n",
       " ('Advertisement\\n\\n\\n\\nBroadly', 'PERSON'),\n",
       " ('Advertisement\\n\\n\\n\\nDuncan', 'PERSON'),\n",
       " ('Advertisement\\n\\n\\n\\nGreta Gladney', 'PERSON'),\n",
       " ('Advertisement\\n\\n\\n\\nMeadows', 'PERSON'),\n",
       " ('Advertisement\\n\\n\\n\\nShakur', 'PERSON'),\n",
       " ('Advertisement\\n\\n\\n\\nSmedley', 'PERSON'),\n",
       " ('Advertisement\\n\\n\\n\\nTrump', 'PERSON'),\n",
       " ('Advertisement\\n\\n\\n\\nVallas', 'PERSON'),\n",
       " ('Advertisement\\n\\n\\n\\nXi', 'PERSON'),\n",
       " ('Anita Dunn', 'PERSON'),\n",
       " ('Antony Blinken', 'PERSON'),\n",
       " ('Arne Duncan', 'PERSON'),\n",
       " ('Ashana Brigard', 'PERSON'),\n",
       " ('Barack Obama', 'PERSON'),\n",
       " ('Beryl Howell', 'PERSON'),\n",
       " ('Biden', 'PERSON'),\n",
       " ('Bill Finch', 'PERSON'),\n",
       " ('Brad Raffensperger', 'PERSON'),\n",
       " ('Brandon Johnson', 'PERSON'),\n",
       " ('Brett Kavanaugh', 'PERSON'),\n",
       " ('Bridgeport', 'PERSON'),\n",
       " ('Chris Pappas', 'PERSON'),\n",
       " ('Chris Sununu', 'PERSON'),\n",
       " ('David Hornbeck', 'PERSON'),\n",
       " ('David Scanlan', 'PERSON'),\n",
       " ('Deb Grill', 'PERSON'),\n",
       " ('Dix Moore-Broussard', 'PERSON'),\n",
       " ('Donald J. Trump', 'PERSON'),\n",
       " ('Donald Trump', 'PERSON'),\n",
       " ('Douglass', 'PERSON'),\n",
       " ('Duncan', 'PERSON'),\n",
       " ('ERIC', 'PERSON'),\n",
       " ('Evan Corcoran', 'PERSON'),\n",
       " ('Fani Willis', 'PERSON'),\n",
       " ('Fran Rabinowitz', 'PERSON'),\n",
       " ('Frances Watson', 'PERSON'),\n",
       " ('Frederick Douglass High School', 'PERSON'),\n",
       " ('Gary Peluchette', 'PERSON'),\n",
       " ('Gladney', 'PERSON'),\n",
       " ('Gloria Warner', 'PERSON'),\n",
       " ('Grill', 'PERSON'),\n",
       " ('Hassan', 'PERSON'),\n",
       " ('J. Christian Adams', 'PERSON'),\n",
       " ('JROTC', 'PERSON'),\n",
       " ('Jack Smith', 'PERSON'),\n",
       " ('Jason Smedley', 'PERSON'),\n",
       " ('Jeffrey Zients', 'PERSON'),\n",
       " ('Joe Biden', 'PERSON'),\n",
       " ('Joe Biden Thinking Lately', 'PERSON'),\n",
       " ('John F. Kennedy', 'PERSON'),\n",
       " ('Johnson', 'PERSON'),\n",
       " ('Jonathan Mitchell', 'PERSON'),\n",
       " ('JudiciaryTweet', 'PERSON'),\n",
       " ('Kevin Durand', 'PERSON'),\n",
       " ('Kim Reynolds', 'PERSON'),\n",
       " ('Kristy Mosby', 'PERSON'),\n",
       " ('Laurie Jasper', 'PERSON'),\n",
       " ('Law Enforcement', 'PERSON'),\n",
       " ('Lawrence Freedman', 'PERSON'),\n",
       " ('Lisa Haver', 'PERSON'),\n",
       " ('Loop', 'PERSON'),\n",
       " ('Maggie Hassan', 'PERSON'),\n",
       " ('Marcus Silva', 'PERSON'),\n",
       " ('Maria Pereira', 'PERSON'),\n",
       " ('Mark Meadows', 'PERSON'),\n",
       " ('Martin Bednarek', 'PERSON'),\n",
       " ('Mary Christianakis', 'PERSON'),\n",
       " ('Matt G. Olson', 'PERSON'),\n",
       " ('Meadows', 'PERSON'),\n",
       " ('Mike Pence', 'PERSON'),\n",
       " ('Mitchell', 'PERSON'),\n",
       " ('Mosby', 'PERSON'),\n",
       " ('Neil Levesque', 'PERSON'),\n",
       " ('Nikita Khrushchev', 'PERSON'),\n",
       " ('Nixon', 'PERSON'),\n",
       " ('Obama', 'PERSON'),\n",
       " ('Open Campus', 'PERSON'),\n",
       " ('Paul Vallas', 'PERSON'),\n",
       " ('Pell Grants', 'PERSON'),\n",
       " ('Peluchette', 'PERSON'),\n",
       " ('Putin', 'PERSON'),\n",
       " ('ROTC', 'PERSON'),\n",
       " ('Rabinowitz', 'PERSON'),\n",
       " ('Rahm Emanuel', 'PERSON'),\n",
       " ('Richard M. Daley', 'PERSON'),\n",
       " ('Richard Nixon', 'PERSON'),\n",
       " ('Ron Klain', 'PERSON'),\n",
       " ('Schneider', 'PERSON'),\n",
       " ('Senn', 'PERSON'),\n",
       " ('Shakur', 'PERSON'),\n",
       " ('Sheila Cohen', 'PERSON'),\n",
       " ('Smith', 'PERSON'),\n",
       " ('Steve Ricchetti', 'PERSON'),\n",
       " ('Tucker Carlson', 'PERSON'),\n",
       " ('Vallas', 'PERSON'),\n",
       " ('Vladimir Putin', 'PERSON'),\n",
       " ('Xi', 'PERSON'),\n",
       " ('Xi Jinping', 'PERSON'),\n",
       " ('pro-Vallas', 'PERSON')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ne_from_corpus(slate_corpus,[\"PERSON\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7f4b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function that gets sentiment (affect) for entities in a list of entites\n",
    "# in a corpus\n",
    "def get_ne_affect_from_corpus(corpus,entity_list):\n",
    "    import nltk\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    from nltk import sent_tokenize\n",
    "    \n",
    "    #We need a running total of the sentiment and the total number of sentences\n",
    "    entity_dict=dict() \n",
    "    for entity in entity_list:\n",
    "        entity_dict[entity[0]] = [0,0] #[sentiment total, number of sentences]\n",
    "        \n",
    "    #Iterate through all the documents\n",
    "    for doc in corpus:\n",
    "        #tokenize one doc\n",
    "        sents = sent_tokenize(doc[1])\n",
    "        \n",
    "        #Iterate through all sentences\n",
    "        #For each entity, check if it is in the sentence\n",
    "        #If it is, add the \"compound\" vader score and a 1 for sentence count to the running total dict\n",
    "        #\n",
    "        for sent in sents:\n",
    "            for ent in entity_dict:\n",
    "                if ent in sent:\n",
    "                    va = analyzer.polarity_scores(sent)\n",
    "                    entity_dict[ent][0] += va['compound']\n",
    "                    entity_dict[ent][1] += 1\n",
    "    #Calculate net sentiment (total compound score/number of sentences) for each entity\n",
    "    sentiments = dict()\n",
    "    for ent,vals in entity_dict.items():\n",
    "        try:\n",
    "            sentiments[ent] = vals[0]/vals[1]\n",
    "        except:\n",
    "            sentiments[ent] = 0.0\n",
    "    return sentiments               \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b1eeefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = get_ne_from_corpus(slate_corpus,[\"PERSON\"])\n",
    "entity_sentiments = get_ne_affect_from_corpus(slate_corpus,entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f2b3f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Richard Nixon': 0.9201,\n",
       " 'Kristy Mosby': 0.8558,\n",
       " 'Donald J. Trump': 0.8274,\n",
       " 'Bill Finch': 0.6597,\n",
       " 'Loop': 0.6369,\n",
       " 'Nixon': 0.63005,\n",
       " 'Advertisement\\n\\n\\n\\nDuncan': 0.608,\n",
       " 'Brett Kavanaugh': 0.5719,\n",
       " 'Jonathan Mitchell': 0.5267,\n",
       " 'Gloria Warner': 0.4939,\n",
       " 'Mosby': 0.447525,\n",
       " 'Steve Ricchetti': 0.4404,\n",
       " 'Anita Dunn': 0.4404,\n",
       " 'Neil Levesque': 0.4215,\n",
       " 'Paul Vallas': 0.4103333333333334,\n",
       " 'Advertisement\\n\\n\\n\\nVallas': 0.39881666666666665,\n",
       " 'Vladimir Putin': 0.3645,\n",
       " 'Duncan': 0.3639333333333334,\n",
       " 'Advertisement\\n\\n\\n\\nShakur': 0.3551,\n",
       " 'David Hornbeck': 0.34,\n",
       " 'Advertisement\\n\\n\\n\\nGreta Gladney': 0.3182,\n",
       " 'Pell Grants': 0.3141,\n",
       " 'Advertisement\\n\\n\\n\\nMeadows': 0.296,\n",
       " 'Senn': 0.22744999999999999,\n",
       " 'Johnson': 0.21230000000000002,\n",
       " 'JROTC': 0.15086666666666668,\n",
       " 'Schneider': 0.14049999999999999,\n",
       " 'Mark Meadows': 0.128,\n",
       " 'Beryl Howell': 0.128,\n",
       " 'Jack Smith': 0.128,\n",
       " 'Open Campus': 0.10389999999999999,\n",
       " 'Mitchell': 0.09084615384615385,\n",
       " 'ROTC': 0.06465714285714286,\n",
       " 'Biden': 0.057699999999999974,\n",
       " 'Donald Trump': 0.024706250000000037,\n",
       " 'Shakur': 0.009742857142857133,\n",
       " 'Meadows': 0.005740000000000003,\n",
       " 'Dix Moore-Broussard': 0.0,\n",
       " 'Maggie Hassan': 0.0,\n",
       " 'J. Christian Adams': 0.0,\n",
       " 'Frances Watson': 0.0,\n",
       " 'Ron Klain': 0.0,\n",
       " 'Gary Peluchette': 0.0,\n",
       " 'Advertisement\\n\\n\\n\\nSmedley': 0.0,\n",
       " 'Sheila Cohen': 0.0,\n",
       " 'Laurie Jasper': 0.0,\n",
       " 'Jeffrey Zients': 0.0,\n",
       " 'Matt G. Olson': 0.0,\n",
       " 'Advertisement\\n\\n\\n\\nBroadly': 0.0,\n",
       " 'Tucker Carlson': 0.0,\n",
       " 'JudiciaryTweet': 0.0,\n",
       " 'Advertisement\\n\\n\\n\\nTrump': 0.0,\n",
       " 'Jason Smedley': 0.0,\n",
       " 'Mary Christianakis': 0.0,\n",
       " 'Maria Pereira': 0.0,\n",
       " 'David Scanlan': 0.0,\n",
       " 'Fani Willis': 0.0,\n",
       " 'Brad Raffensperger': 0.0,\n",
       " 'Kevin Durand': 0.0,\n",
       " 'Ashana Brigard': 0.0,\n",
       " 'Richard M. Daley': 0.0,\n",
       " 'Frederick Douglass High School': 0.0,\n",
       " 'pro-Vallas': 0.0,\n",
       " 'Brandon Johnson': 0.0,\n",
       " 'ERIC': -0.0010153846153846337,\n",
       " 'Smith': -0.006900000000000002,\n",
       " 'Barack Obama': -0.0086,\n",
       " 'Vallas': -0.01358500000000001,\n",
       " 'Putin': -0.01480000000000001,\n",
       " 'Xi': -0.018428571428571412,\n",
       " 'Joe Biden': -0.041296,\n",
       " 'Douglass': -0.053671428571428585,\n",
       " 'Advertisement\\n\\n\\n\\n': -0.05630445859872609,\n",
       " 'Mike Pence': -0.07145000000000003,\n",
       " 'Chris Sununu': -0.07655,\n",
       " 'Bridgeport': -0.0944,\n",
       " 'Lisa Haver': -0.11315,\n",
       " 'Obama': -0.11696923076923078,\n",
       " 'Arne Duncan': -0.1531,\n",
       " 'Hassan': -0.1806,\n",
       " 'Grill': -0.18737499999999999,\n",
       " 'Gladney': -0.1932,\n",
       " 'Law Enforcement': -0.273,\n",
       " 'Martin Bednarek': -0.2732,\n",
       " 'Advertisement\\n\\n\\n\\nXi': -0.2833,\n",
       " 'Joe Biden Thinking Lately': -0.2936083333333333,\n",
       " 'Peluchette': -0.3050857142857143,\n",
       " 'Fran Rabinowitz': -0.3182,\n",
       " 'Rabinowitz': -0.3182,\n",
       " 'Nikita Khrushchev': -0.3612,\n",
       " 'Evan Corcoran': -0.4019,\n",
       " 'Xi Jinping': -0.4019,\n",
       " 'Deb Grill': -0.4019,\n",
       " 'Lawrence Freedman': -0.4215,\n",
       " 'Chris Pappas': -0.624,\n",
       " 'Antony Blinken': -0.6249,\n",
       " 'Rahm Emanuel': -0.6705,\n",
       " 'John F. Kennedy': -0.6705,\n",
       " 'Kim Reynolds': -0.7351,\n",
       " 'Marcus Silva': -0.9337}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(entity_sentiments.items(), key=lambda item: -1*item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "840e1897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Joe Biden': -0.041296, 'Donald Trump': 0.024706250000000037}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ne_affect_from_corpus(slate_corpus,[[\"Joe Biden\",'PERSON'],[\"Donald Trump\",'PERSON']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aaf160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
