{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Decision Trees and Machine Learning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Decision trees are tree structures containing rules</li>\n",
    "<li>The leaf nodes of the tree are the \"learned\" categories (or threshold values)</li>\n",
    "<li>A path from the root to a leaf node represents a rule (or a decision path)</li>\n",
    "<li>Leaf nodes represent predictions (either a category or the mean (expected) value of a decision variable</li>\n",
    "<li>Each case filters through the tree from the root to a leaf node to get a prediction</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Why decision trees?</span>\n",
    "<li>Easy to understand </li>\n",
    "<li>Rule finding process is transparent</li>\n",
    "<li>Can handle \"mixed\" categorical(male/female) and numerical (age, number of siblings) data</li>\n",
    "<li>Can handle missing data </li>\n",
    "<li>Can be used to generate partial \"good\" solutions</li>\n",
    "<li>Can find non-linear patterns in the data</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Decision trees and non-linear patterns</span>\n",
    "<p></p>\n",
    "<img src=\"linear_model.png\">\n",
    "<img src=\"linear_classifier_accuracy.png\">\n",
    "<img src=\"non_linear_classifier.png\">\n",
    "<img src=\"decision_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Why not decision trees?</span>\n",
    "<li>Finding an optimal tree is a hard problem</li>\n",
    "<li>Overfitting is a problem</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Example: Predicting wine quality</h1>\n",
    "<li><span style=\"color:blue\">Input features</span>: Chemical properties of wines</li>\n",
    "<li><span style=\"color:blue\">Wine quality</span>: A number between 0 and 10</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">Import the data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1599 non-null   float64\n",
      " 1   volatile acidity      1599 non-null   float64\n",
      " 2   citric acid           1599 non-null   float64\n",
      " 3   residual sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free sulfur dioxide   1599 non-null   float64\n",
      " 6   total sulfur dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  alcohol               1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    }
   ],
   "source": [
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "w_df = pd.read_csv(url,header=0,sep=';')\n",
    "w_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">Examine the dependent variable</span>\n",
    "<li>Higher dv values indicate a better quality wine</li>\n",
    "<li>Lower dv values indicate a poorer quality wine</li>\n",
    "<li>We'll assume that the values are continuous</li>\n",
    "<li>And use the various features to predict wine quality</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_df['quality'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">Build train and test samples</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(w_df, test_size = 0.3)\n",
    "x_train_wine = train.iloc[0:,0:11]\n",
    "y_train_wine = train[['quality']]\n",
    "x_test_wine = test.iloc[0:,0:11]\n",
    "y_test_wine = test[['quality']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Choose a model</span>\n",
    "<p></p>\n",
    "<ul>\n",
    "<li><b>Classification trees</b>: Uses rules to classify cases into two or more categories (classify handwritten digits)</li>\n",
    "<ul>\n",
    "<li>Classification trees recursively split the data on a feature value</li>\n",
    "<li>Each split minimizes the entropy (also known as the impurity)</li>\n",
    "<li>Entropy is commonly measured using the GINI cost function (a measure of the probability of misclassification or 'purity')</li>\n",
    "    <li>Classifiers are used when the target variable is a set of unordered categories (handwritten digit recognition)</li>\n",
    "</ul>\n",
    "<li><b>Regression trees</b>: Uses rules to group data into target variable ranges (Wine Quality)</li>\n",
    "<ul>\n",
    "<li>Also split the data on feature values</li> \n",
    "    <li>Minimize cost (impurity). Usually the mean squared error</li>\n",
    "    <li>Regression trees are used when the target variable is continuous and ordered (wine quality from 0 to 10)</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">Regression trees</span>\n",
    "<li>Run regressions for each X to the dependent variable</li>\n",
    "<li>Pick the variable with the most explanatory power and split it at several points</li>\n",
    "<li>Calculate the Mean Square Error of each of the two halves for each split</li>\n",
    "<li>Pick the split point that gives the lowest mse (combined)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Decision trees and Entropy minimization</span>\n",
    "<p></p>\n",
    "<li><b>Entropy</b>: a measure of uncertainty in the data<p></li>\n",
    "<ul>\n",
    "    <li>what is the uncertainty in color when you draw a marble from a box of 100 blue marbles?</li>\n",
    "    <li>what is the uncertainty when you draw a marble from a box with 50 blue and 50 red marbles?</li>\n",
    "</ul>\n",
    "<li>Entropy minimization: decision tree algorithms seek to partition the data on features in a way so that total entropy is minimized</li>\n",
    "<li>In the case of regression trees, mean square error serves as a proxy for entropy</li>\n",
    "<li>In the case of decision trees, gini, a measure of the frequency of misclassification of an element, is used as a proxy for entropy (you can also directly measure entropy but it is computationally inefficient to do so)</li>\n",
    "<p></p>\n",
    "<span style=\"color:red;font-size:large\">The danger of entropy minimization</span>\n",
    "<p></p>\n",
    "<li>In the degenerate case, we can build rules that partition the data into single case subsets</li>\n",
    "<li>The resulting combined entropy will be zero!</li>\n",
    "<li>But the results will be useless because we will likely not be able to predict anything if we get a new case</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">Tree depth: Stopping and Pruning Rules</span>\n",
    "<li>In the degenerate case, a decision tree algorithm can build a tree with exactly one training case in each leaf node</li>\n",
    "<li>This would be pointlessly overfitted!</li>\n",
    "<p></p>\n",
    "<span style=\"color:green;font-size:x-large\">controlling for overfitting</span>\n",
    "\n",
    "<li>Set a minimum count of observations in each leaf node. If the number of observations falls below the minimum, don't split the node any further</li>\n",
    "<li>Set a maximum tree <b>depth</b>. Once a path reaches a certain length, stop splitting that path</li>\n",
    "<li>Minimize <b>complexity cost</b>. Complexity cost in a decision tree is a function of the overall misclassification rate of the tree (we want the overall misclasification rate to be low) and the number of leaf nodes (we don't want too many categories because of the overfitting danger)</li>\n",
    "<li>Various other parameters (cf. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</a>)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "wine_rgr = tree.DecisionTreeRegressor(max_depth=3,min_samples_leaf=20,min_samples_split=50)\n",
    "wine_rgr.fit(x_train_wine,y_train_wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">Evaluating a regressor</span>\n",
    "<li>Since a regressor is predicting continuous values of the dv, we can evaluate it like a linear regression model</li>\n",
    "<li><span style=\"color:blue\">R-Square</span> tells us how much of the variance in the data is explained by our model (how well the model fits the data)</li>\n",
    "<li><span style=\"color:blue\">mean square error</span> gives us an estimate of how far, on the average, are our predictions from actuals (this is better as a model comparison tool)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the R-Square for the predicted vs actuals on the text sample\n",
    "print(\"Training R-Square\",wine_rgr.score(x_train_wine,y_train_wine))\n",
    "print(\"Testing R-Square\",wine_rgr.score(x_test_wine,y_test_wine))\n",
    "#print(\"Training mean sq error\",wine_rgr.score(x_train_wine,y_train_wine))\n",
    "#print(\"Testing mean sq error\",wine_rgr.score(x_test_wine,y_test_wine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">Visualizing the tree</span>\n",
    "<li>sklearn has a handy function <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html\">plot_tree</a> for visualizing a tree</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#associate column names with features\n",
    "feature_names = w_df.columns[:-1]\n",
    "\n",
    "#set up a figure (mainly for the size)\n",
    "fig, ax = plt.subplots(figsize=(24, 28))\n",
    "\n",
    "#plot the tree\n",
    "tree.plot_tree(wine_rgr,feature_names=feature_names, max_depth=4, fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Classification trees</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:green;font-size:xx-large\">In-class problem: Classifying wine into good or bad wine</span>\n",
    "<li>assume any wine with quality less than 5.5 is a bad wine</li>\n",
    "<li>and any wine with quality greater than or equal to 5.5 is a good wine</li>\n",
    "<li>build a classifier that classifies the wine data into good and bad wines</li>\n",
    "<li>use the training data to build the classifier and report the accuracy <span style=\"color:red\">score</span> on the testing data</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build train and test data\n",
    "x_train_wine = train.iloc[0:,0:11]\n",
    "y_train_wine = train[['quality']]\n",
    "x_test_wine = test.iloc[0:,0:11]\n",
    "y_test_wine = test[['quality']]\n",
    "\n",
    "#Convert y values into categorical data\n",
    "y_train_wine_cat = y_train_wine >= 5.5\n",
    "y_test_wine_cat = y_test_wine >= 5.5\n",
    "\n",
    "#Set up and fit a model \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "wine_clf = DecisionTreeClassifier(max_depth=4)\n",
    "#fit the data to the classifier\n",
    "wine_clf.fit(x_train_wine,y_train_wine_cat)\n",
    "\n",
    "#Report the accuracy score\n",
    "training_accuracy = wine_clf.score(x_train_wine,y_train_wine_cat)\n",
    "testing_accuracy = wine_clf.score(x_test_wine,y_test_wine_cat)\n",
    "print(training_accuracy)\n",
    "print(testing_accuracy)\n",
    "\n",
    "#Render the decision tree\n",
    "import matplotlib.pyplot as plt\n",
    "feature_names = w_df.columns[:-1]\n",
    "fig, ax = plt.subplots(figsize=(24, 28))\n",
    "tree.plot_tree(wine_clf,feature_names=feature_names, max_depth=4, fontsize=12)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
