{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:red;font-size:60px\">Text Understanding</span>\n",
    "<br><br>\n",
    "<li>sentiment analysis tells us whether a document/sentence/word is positive or negative</li>\n",
    "<li>and is useful when comparing documents</li>\n",
    "<li>Often, however, we want to understand something about the content of a document or corpus</li>\n",
    "<li>This is a cutting edge area of research that works on:</li>\n",
    "<ul>\n",
    "    <li><span style=\"color:blue\">Topic Analysis</span>: identifying major topics or themes in a corpus and then relating a document to these topics</li>\n",
    "    <li><span style=\"color:blue\">Document similarity</span>: finding the closest other document to a document</li>\n",
    "    <li><span style=\"color:blue\">Document summarization</span>: generating a meaningful summary of a document</li>\n",
    "    <li><span style=\"color:blue\">Knowledge graphs</span>: building a graph of objects and their relationships in a corpus and then using this graph to \"reason\" about the objects and relationships in a new document</li>\n",
    "    <li><span style=\"color:blue\">Translation</span>: constructing document coders and decoders to translate from one language to another</li>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:40px\">Preparing a corpus</span>\n",
    "<p></p>\n",
    "<li>Documents and corpora are words but, for any meaningful analysis, we need to deal with numbers</li>\n",
    "<li>The first step is to convert this \"corpus of words\" into a \"corpus of numbers\"</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<span style=\"color:green;font-size:30px\">Bag of Words Model</span>\n",
    "<p></p>\n",
    "<li><span style=\"color:blue\">Vocabulary</span> refers to the superset of all words in the corpus</li>\n",
    "<li>Each word in the vocabulary can be associated with a unique integer</li>\n",
    "<li>A <span style=\"color:blue\">bag of words</span> is a JSON style dictionary that contains each word in the corpus as the key, and a unique integer as the value</li>\n",
    "<li>A <span style=\"color:blue\">word vector</span> is a vector of the same length as the vocabulary. Each document can be represented as a word vector by entering the number of occurrences of the word in its corresponding vocabulary</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Vocabulary example</span>\n",
    "<p></p>\n",
    "<li>tokenize the text</li>\n",
    "<li>create a list of all the tokens</li>\n",
    "<li>the index of a word in the list is its numerical equivalent</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patriot': 0,\n",
       " 'break': 1,\n",
       " ',': 2,\n",
       " 'as': 3,\n",
       " 'went': 4,\n",
       " 'flag': 5,\n",
       " 'took': 6,\n",
       " 'a': 7,\n",
       " 'buy': 8,\n",
       " 'American': 9,\n",
       " 'are': 10,\n",
       " 'Habits': 11,\n",
       " 'take': 12,\n",
       " 'Jack': 13,\n",
       " 'to': 14,\n",
       " 'on': 15,\n",
       " 'his': 16,\n",
       " 'banana': 17,\n",
       " 'hard': 18,\n",
       " 'likes': 19,\n",
       " 'Jill': 20,\n",
       " 'Boston': 21,\n",
       " 'train': 22,\n",
       " 'patriotic': 23}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "s1 = \"Jack, a patriot, went to Boston to buy a banana\"\n",
    "s2 = \"Jill likes to take a banana to Boston on a train\"\n",
    "s3 = \"Habits are hard to break\"\n",
    "s4 = \"Jack, as a patriotic American, took his flag to Boston\"\n",
    "\n",
    "words = list(set(nltk.word_tokenize(s1 + \" \" + s2 + \" \" + s3 + \" \" + s4)))\n",
    "vocab = {v:k for k,v in enumerate(words)}\n",
    "vocab  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Word vector example</span>\n",
    "<p></p>\n",
    "<li>Each document or sentence (the unit of analysis) is converted into a word vector</li>\n",
    "<li>Create a list of the same length as the vocabulary with all values = 0</li>\n",
    "<li>word tokenize the document</li>\n",
    "<li>iterate through the tokens, find the associated index of the token in the vocab, increment the count by one in the word vector</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0]*len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 2, 1, 0, 1, 0, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Functionalizing\n",
    "#s2 = \"Jill likes to take a banana to Boston on a train\"\n",
    "def get_word_vec(sentence,vocab):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    vec = [0]*len(vocab)\n",
    "    for word in words:\n",
    "        ind = vocab[word]\n",
    "        vec[ind]+=1\n",
    "    return vec\n",
    "get_word_vec(s2,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 0, 1, 0, 0, 2, 1, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 2, 1, 0, 1, 0, 1, 1, 1, 1, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 2, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "s1 = \"Jack, a patriot, went to Boston to buy a banana\"\n",
    "s2 = \"Jill likes to take a banana to Boston on a train\"\n",
    "s3 = \"Habits are hard to break\"\n",
    "s4 = \"Jack, as a patriotic American, took his flag to Boston\"\n",
    "\n",
    "s1_vec = get_word_vec(s1,vocab)\n",
    "s2_vec = get_word_vec(s2,vocab)\n",
    "s3_vec = get_word_vec(s3,vocab)\n",
    "s4_vec = get_word_vec(s4,vocab)\n",
    "print(s1_vec,s2_vec,s3_vec,s4_vec,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Cleaning the vocabulary</span>\n",
    "<p></p>\n",
    "<li>The vocabulary is the starting point of the analysis so getting it right is important</li>\n",
    "<li>Including all the words will result in a large number of <span style=\"color:blue\">sparsely populated vectors</span></li>\n",
    "<li>Trying to limit the vocabulary to only useful words is an important, often subjective, preprocessing step</li>\n",
    "<li>Some general approaches</li>\n",
    "<ul>\n",
    "    <li>Get rid of general words that have little interpretive value (\"to\", \"a\", \"on\", \"are\")</li>\n",
    "    <li>Create your own vocabulary of useful words</li>\n",
    "    <li>Convert everything to lowercase so that case differences are not treated as differences</li>\n",
    "    <li>Use a <span style=\"color:blue\">stemming algorithm</span> to reduce words to their stem words (<span style=\"color:red\">having</span>, <span style=\"color:red\">have</span>, <span style=\"color:red\">had</span> are all stemmed as <span style=\"color:red\">have</span>)</li>\n",
    "    <li>Remove punctuation or words that contain non-alpha characters</li>\n",
    "    <li>Remove very short words (2, 3 letters) if that makes sense in your domain</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Removing stopwords</span>\n",
    "<p></p>\n",
    "<li>nltk provides a list of common words</li>\n",
    "<li>Often, you can use your own list of unimportant words</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#Create a list of unimportant words (often supplied as STOPWORDS)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['patriot',\n",
       " 'break',\n",
       " ',',\n",
       " 'went',\n",
       " 'flag',\n",
       " 'took',\n",
       " 'buy',\n",
       " 'American',\n",
       " 'Habits',\n",
       " 'take',\n",
       " 'Jack',\n",
       " 'banana',\n",
       " 'hard',\n",
       " 'likes',\n",
       " 'Jill',\n",
       " 'Boston',\n",
       " 'train',\n",
       " 'patriotic']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a list of unimportant words (often supplied as STOPWORDS)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "[w for w in set(nltk.word_tokenize(s1 + \" \" + s2 + \" \" + s3 + \" \" + s4)) if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Removing punctuation and convert to lower case</span>\n",
    "<p></p>\n",
    "<li>The function <span style=\"color:blue\">isalpha</span> returns True if all characters in a string are letters</li>\n",
    "<li>The function <span style=\"color:blue\">lower</span> returns the lowercase version of a string</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['patriot',\n",
       " 'break',\n",
       " 'went',\n",
       " 'flag',\n",
       " 'took',\n",
       " 'buy',\n",
       " 'american',\n",
       " 'habits',\n",
       " 'take',\n",
       " 'jack',\n",
       " 'banana',\n",
       " 'hard',\n",
       " 'likes',\n",
       " 'jill',\n",
       " 'boston',\n",
       " 'train',\n",
       " 'patriotic']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = [w.lower() for w in set(nltk.word_tokenize(s1 + \" \" + s2 + \" \" + s3 + \" \" + s4)) if w not in stopwords.words('english') and w.isalpha()]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Use word stems and lemmas</span>\n",
    "<p></p>\n",
    "<li><span style=\"color:blue\">stemming</span> is the process of chopping off extraneous letters from a word to get to its root</li>\n",
    "<li>For example, patriotic and patriotism both have patriot as their root</li>\n",
    "<li><span style=\"color:blue\">lemmatization</span> is the process of converting a word into its base form</li>\n",
    "<li>For example, take and took are both forms of the base verb take</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Lemmas</span>\n",
    "<p></p>\n",
    "<li>Lemmatization using spaCy</li>\n",
    "<li>Note that words are converted to their base form but not stemmed</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack 9948091532251288780 Jack\n",
      "the 7425985699627899538 the\n",
      "patriot 15964855605178703125 patriot\n",
      "takes 6789454535283781228 take\n",
      "his 2661093235354845946 his\n",
      "mom 13669990404442017371 mom\n",
      "who 3876862883474502309 who\n",
      "took 6789454535283781228 take\n",
      "her 4115755726172261197 her\n",
      "rollerblades 17471066303817376993 rollerblade\n",
      "to 3791531372978436496 to\n",
      "patriotic 5572050039450085576 patriotic\n",
      "Boston 4866123274559977282 Boston\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Jack the patriot takes his mom who took her rollerblades to patriotic Boston\")\n",
    "for token in doc:\n",
    "    print(token, token.lemma, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jack',\n",
       " 'patriot',\n",
       " 'go',\n",
       " 'boston',\n",
       " 'buy',\n",
       " 'banana',\n",
       " 'jill',\n",
       " 'like',\n",
       " 'take',\n",
       " 'banana',\n",
       " 'boston',\n",
       " 'train',\n",
       " 'habit',\n",
       " 'hard',\n",
       " 'break',\n",
       " 'jack',\n",
       " 'patriotic',\n",
       " 'american',\n",
       " 'take',\n",
       " 'flag',\n",
       " 'boston']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(s1 + \" \" + s2 + \" \" + s3 + \" \" + s4)\n",
    "words = [w.lemma_ for w in doc]\n",
    "vocab = [w.lower() for w in words if w not in stopwords.words('english') and w.isalpha()]\n",
    "vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Stemming</span>\n",
    "<p></p>\n",
    "<li>nltk has several stemming algorithms</li>\n",
    "<li>we'll use porter stemmer</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patriot\n",
      "patriot\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "print(p_stemmer.stem(\"Patriotic\".lower()))\n",
    "print(p_stemmer.stem(\"Patriotism\".lower()))\n",
    "print(p_stemmer.stem(\"his\".lower())) #Sometimes, this can be weird! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'american',\n",
       "  'banana',\n",
       "  'boston',\n",
       "  'break',\n",
       "  'buy',\n",
       "  'flag',\n",
       "  'go',\n",
       "  'habit',\n",
       "  'hard',\n",
       "  'hi',\n",
       "  'jack',\n",
       "  'jill',\n",
       "  'like',\n",
       "  'patriot',\n",
       "  'take',\n",
       "  'train'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "p_stemmer = PorterStemmer()\n",
    "doc = nlp(s1 + \" \" + s2 + \" \" + s3 + \" \" + s4)\n",
    "words = [p_stemmer.stem(w.lemma_) for w in doc]\n",
    "vocab = [set(w.lower() for w in words if w not in stopwords.words('english') and w.isalpha())]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Remove very short words</span>\n",
    "<p></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'american',\n",
       "  'banana',\n",
       "  'boston',\n",
       "  'break',\n",
       "  'buy',\n",
       "  'flag',\n",
       "  'habit',\n",
       "  'hard',\n",
       "  'jack',\n",
       "  'jill',\n",
       "  'like',\n",
       "  'patriot',\n",
       "  'take',\n",
       "  'train'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "p_stemmer = PorterStemmer()\n",
    "doc = nlp(s1 + \" \" + s2 + \" \" + s3 + \" \" + s4)\n",
    "words = [p_stemmer.stem(w.lemma_) for w in doc]\n",
    "vocab = [set(w.lower() for w in words if w not in stopwords.words('english') and w.isalpha() and len(w)>2)]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:xx-large\">Creating word vectors</span>\n",
    "<p></p>\n",
    "<li>from each document include only words, after lemmatization and stemming, that are in the vocabulary</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">A function that cleans documents and generates vocab</span>\n",
    "<p></p>\n",
    "<li>stem and lemmatize all words in each document</li>\n",
    "<li>converts everything to lower case</li>\n",
    "<li>removes stopwords and words less than a certain length</li>\n",
    "<li>removes non-alpha words (and punctuation)</li>\n",
    "<li>each document then becomes a list of acceptable words (in sequence)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"Jack, a patriot, went to Boston to buy a banana\"\n",
    "s2 = \"Jill likes to take a banana to Boston on a train\"\n",
    "s3 = \"Habits are hard to break\"\n",
    "s4 = \"Jack, as a patriotic American, took his flag to Boston\"\n",
    "corpus = [('s1',s1),('s2',s2),('s3',s3),('s4',s4)]\n",
    "domain_stop_words = {\"american\"}\n",
    "def stem_and_lemmatize_corpus(corpus,min_length=3):\n",
    "    import spacy\n",
    "    import nltk\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    from nltk.corpus import stopwords\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    #Clean up the documents\n",
    "    new_doc_list = list()\n",
    "    for document in corpus:\n",
    "        doc = nlp(document[1])\n",
    "        words = [p_stemmer.stem(w.lemma_).lower() for w in doc]    \n",
    "        print(words)\n",
    "        words = [w for w in words if len(w)>min_length and w.isalpha() and w not in stopwords.words(\"english\") and w not in domain_stop_words]\n",
    "        new_doc_list.append((document[0],words))\n",
    "    docs_only = [w for w in [v[1] for v in new_doc_list]]\n",
    "    words = list({w for item in docs_only for w in item})\n",
    "    vocab = {v:k for k,v in enumerate(words)}\n",
    "\n",
    "    return new_doc_list,vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus, vocab = stem_and_lemmatize_corpus(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Generate the word vectors</span>\n",
    "<p></p>\n",
    "<li>Modify our get_word_vec function so that it takes a corpus as an input and outputs a set of word vectors, one per document</li>\n",
    "<li><span style=\"color:red\">Finally!</span> Our text is ready for analysis!</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vectors(corpus,vocab):\n",
    "    vector_list = list()\n",
    "    for doc in corpus:\n",
    "        vec = [0] * len(vocab)\n",
    "        for word in doc[1]:\n",
    "            try:\n",
    "                ind = vocab[word]\n",
    "                vec[ind]+=1\n",
    "            except:\n",
    "                pass    \n",
    "        vector_list.append([doc[0],vec]) \n",
    "    return vector_list\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = get_word_vectors(clean_corpus,vocab)\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:large\">Why do all this?</span>\n",
    "<p></p>\n",
    "<li>Bottom line: Analytics must be done on numbers not on text</li>\n",
    "<li>Too many unnecessary words can be confusing</li>\n",
    "<li>Words that are alike, but differ marginally should be treated as the same word</li>\n",
    "<li>Words that have the same meaning should be treated as the same word</li>\n",
    "\n",
    "<li>Example: similarity between sentences (<a href=\"https://en.wikipedia.org/wiki/Cosine_similarity\">cosine similarity</a>)</li>\n",
    "<li>We can easily figure out that s1 is more similar to s4 than it is to s3</li>\n",
    "<li><b>Downside</b>: While this makes analysis possible, the order of words, which is possibly valuable information, is discarded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"Jack, a patriot, went to Boston to buy a banana\"\n",
    "s2 = \"Jill likes to take a banana to Boston on a train\"\n",
    "s3 = \"Habits are hard to break\"\n",
    "s4 = \"Jack, as a patriotic American, took his flag to Boston\"\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity([word_vectors[0][1],word_vectors[1][1],word_vectors[2][1],word_vectors[3][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "<span style=\"color:red;font-size:60px\">Topic modeling</span>\n",
    "<br><br>\n",
    "<p></p>\n",
    "<li>The goal of topic modeling is to identify the major concepts underlying a piece of text\n",
    "<li>Topic modeling uses \"Unsupervised Learning\". No a-priori knowledge is necessary\n",
    "<li>Though, a-priori knowledge is important for contextualizing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:30px\">LDA: Latent Dirichlet Allocation</span>\n",
    "<p></p>\n",
    "<li>A technique for topic modeling\n",
    "<li>Basic assumptions:\n",
    "<ol>\n",
    "    <li>a document is associated with several topics</li>\n",
    "    <li>the topics vary in importance</li>\n",
    "    <li>documents are constructed by drawing words from a universe of words (aka \"bag of words\" or vocab)</li>\n",
    "    <li>topics are distributed across a probability distribution of words from this bag of words</li>\n",
    "    <li>each topic is represented in the document by the distribution of words associated with the topic in the document </li>\n",
    "\n",
    "</ol>\n",
    "<li>Given these assumptions, LDA scans a corpus and tries to deduce the topic and word distributions in the corpus</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Example</span>\n",
    "<li>Document 1: (dog, cat, rat, and, the, goat)</li>\n",
    "<li>Document 2: (dog, chair, table, and, the, bed)</li>\n",
    "<li>Document 3: (dog, cat, chair, bed, the, goat)</li>\n",
    "\n",
    "Assuming 2 topics, t1 and t2, LDA builds the following associations:\n",
    "<p>\n",
    "    <img src=\"lsa.png\">\n",
    "    </p>\n",
    "\n",
    "<li>Each topic is then defined by the top n words in order of probabilities</li>\n",
    "<li>And each document is associated with the high probability probabilities from documents to topics</li>\n",
    "<li>The details of LDA are not in the scope of this class but, if interested, read:</li>\n",
    "<ul>\n",
    "    <li><a href=\"https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf\">the original paper</a></li>\n",
    "    <li><a href=\"https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\">this article on medium</a></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:x-large\">term frequency (tf) and inverse document frequency (idf)</span>\n",
    "<li><span style=\"color:red\">term frequency</span>: term frequency refers to the number of times a word occurs in a document</li>\n",
    "<li><span style=\"color:red\">inverse document frequency</span>: refers to the number of documents that contain a word</li>\n",
    "<li>The idea is that the more often a document references a word, the more likely that word will be useful in identifying a topic</li>\n",
    "<li>But, the main purpose of topic analysis is to group documents into <b>different</b> topics. A word that occurs in many documents in a corpus is less likely to be useful in discriminating between documents</li>\n",
    "<li>tf-idf methodologies increase the liklihood of a word belonging to a topic if it is frequent in a document but decrease that liklihood if the word is freqently used in many documents</li>\n",
    "<li>Example:</li>\n",
    "<ul>\n",
    "    <li>Document 1: (dog, cat, rat, ate, the, cat)</li>\n",
    "    <li>Document 2: (dog, chair, table, and, the, chair)</li>\n",
    "    <li>Document 3: (dog, cat, chair, chased, the, cat)</li>\n",
    "    <li>\"dog\" occurs in all documents. It's not going to be helpful in discriminating between documents</li>\n",
    "    <li>\"cat\" occurs twice in documents 1 and 3. It is likely to be a topic identifier</li>\n",
    "    <li>\"chair\" occurs twice in one document. It is also likely to be a topic identifier</li>\n",
    "</ul>\n",
    "<li>Most topic analysis techniques use the basic ideas of tf-idf in some way when finding topics</li>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Slate stories topics</span>\n",
    "<p></p>\n",
    "<li>Let's examine the major topics in the news by extracting them from our slate corpus</li>\n",
    "<li>Get the stories by scraping slate.com</li>\n",
    "<li>Get the vocabulary</li>\n",
    "<li>We'll use gensim's own word vector model rather than ours</li>\n",
    "<li>Train the LDA model</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     57
    ]
   },
   "outputs": [],
   "source": [
    "def get_slate_stories():\n",
    "    #followable_links contains the links to news and politics stories\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    url=\"https://www.slate.com\"\n",
    "    page = requests.get(url)\n",
    "    bs_page = BeautifulSoup(page.content,'lxml')\n",
    "    all_links = bs_page.find_all('a')\n",
    "    \n",
    "    #Define the list of categories that we want to follow\n",
    "    categories = ['news-and-politics','business','technology','culture']\n",
    "    \n",
    "    #followable_links will contain the title and the detail of each story\n",
    "    followable_links = set()\n",
    "    for link in all_links:\n",
    "        href = link.get('href') #get the link\n",
    "        if href: #If the link exists (sometimes it doesn't!)\n",
    "            for cat in categories:\n",
    "                if cat in href: #Only stories in the category\n",
    "                    title = link.get_text().strip() #Get the story title\n",
    "                    followable_links.add((title,href)) #Append (title, link) to followable links\n",
    "    \n",
    "    \n",
    "    #Iterate through followable links extracting the text of each story\n",
    "    #story_list is a list of the stories\n",
    "    #Note that some links will not contain an article_body section, those will be ignored (that's why the try except)\n",
    "    story_list = list()\n",
    "    count=0\n",
    "    for link in followable_links:\n",
    "        try:\n",
    "            page=BeautifulSoup(requests.get(link[1]).content,'lxml')\n",
    "            text=page.find('body').find('section',class_='article__body').get_text().strip()\n",
    "            story_list.append((link[0],text))\n",
    "            count+=1\n",
    "        except:\n",
    "            continue\n",
    "    return story_list\n",
    "\n",
    "def lemmatize_corpus(corpus,min_length=3):\n",
    "    import spacy\n",
    "    import nltk\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    from nltk.corpus import stopwords\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    p_stemmer = PorterStemmer()\n",
    "    domain_stop_words = {\"advertisement\",\"medium\",\"somewhere\",\"somewhat\",\"america\"}\n",
    "\n",
    "    #Clean up the documents\n",
    "    new_doc_list = list()\n",
    "    for document in corpus:\n",
    "        doc = nlp(document[1])\n",
    "        #We won't stem the words\n",
    "        words = [(w.lemma_).lower() for w in doc]    \n",
    "        words = [w for w in words if len(w)>min_length and w.isalpha() and w not in stopwords.words(\"english\") and w not in domain_stop_words]\n",
    "        new_doc_list.append((document[0],words))\n",
    "    docs_only = [w for w in [v[1] for v in new_doc_list]]\n",
    "    words = list({w for item in docs_only for w in item})\n",
    "    vocab = {v:k for k,v in enumerate(words)}\n",
    "\n",
    "    return new_doc_list,vocab\n",
    "\n",
    "def get_word_vectors(corpus,vocab):\n",
    "    vector_list = list()\n",
    "    for doc in corpus:\n",
    "        vec = [0] * len(vocab)\n",
    "        for word in doc[1]:\n",
    "            try:\n",
    "                ind = vocab.index(word)\n",
    "                vec[ind]+=1\n",
    "            except:\n",
    "                pass    \n",
    "        vector_list.append([doc[0],vec]) \n",
    "    return vector_list\n",
    "\n",
    "    \n",
    "slate_corpus = get_slate_stories()\n",
    "clean_slate_corpus,vocab = lemmatize_corpus(slate_corpus,5)\n",
    "slate_vectors = get_word_vectors(clean_slate_corpus,vocab) #We won't use this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">LDA Model:</span>\n",
    "<p></p>\n",
    "<li>Number of topics: The number of topics you want generated. \n",
    "<li>Passes: The LDA model makes through the document. More passes, slower analysis\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Document 1: (dog, cat, rat, and, the, goat)\n",
    "Document 2: (dog, chair, table, and, the, bed)\n",
    "Document 3: (dog, cat, chair, bed, the, goat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Rough procedure</span>\n",
    "<p></p>\n",
    "\n",
    "<li>LDA first randomly assigns words to each topic</li>\n",
    "<li>Then it computes $p(t|d)$ - the probability that a topic is associated with a document (the percentage of document d words that are in t)</li>\n",
    "<li>And computes $p(w|t)$ - the probability that a word is associated with a topic (the percentage of documents that contain the word/topic combination</li>\n",
    "<li>Then, if $p(t1|d1) * p(w|t1) < p(t2|d1) * p(w|t2)$, LDA switches the word from topic 1 to topic 2 in document 1 for all word/document combinations</li>\n",
    "<li>And repeats for n passes</li>\n",
    "<li>lda provides these two probabilities =$p(t|d)$ and $p(w|t)$ at the end of the passes</li>\n",
    "<p></p>\n",
    "<img src=\"lda.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/leo/anaconda3/lib/python3.9/site-packages (4.1.2)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.7.0 in /home/leo/anaconda3/lib/python3.9/site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/leo/anaconda3/lib/python3.9/site-packages (from gensim) (1.24.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/leo/anaconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\n",
      "Installing collected packages: gensim\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.1.2\n",
      "    Uninstalling gensim-4.1.2:\n",
      "      Successfully uninstalled gensim-4.1.2\n",
      "Successfully installed gensim-4.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gensim uses a dictionary object that maps words and integers (the vocabulary)\n",
    "#An object with keys = integers, values = words\n",
    "#the dictionary is useful for reverse lookup, i.e., going from a word number to the actual word</li>\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary \n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "slate_texts = [d[1] for d in clean_slate_corpus] #extract the texts into a list (get rid of titles)\n",
    "slate_dictionary = Dictionary(slate_texts) #Create the vocab dictionary\n",
    "slate_topic_corpus = [slate_dictionary.doc2bow(doc) for doc in slate_texts] #(word, frequency) pairs for each doc\n",
    "lda = LdaModel(slate_topic_corpus,id2word=slate_dictionary,num_topics=5,passes=10) #build the lda model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Result: topic word probabilities</span>\n",
    "<p></p>\n",
    "<li>print_topics returns the num_words most probable words associated with a topic</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(lda.print_topics(num_words=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Result: topic document probabilities</span>\n",
    "<p></p>\n",
    "<li>get_document_topics returns the probability distribution of topics for a given document (vectorized)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "lda.get_document_topics(slate_topic_corpus[1],minimum_probability=0.05,per_word_topics=False)\n",
    "sorted(lda.get_document_topics(slate_topic_corpus[1],minimum_probability=0,per_word_topics=False),key=itemgetter(1),reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slate_corpus[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Applying the model to a new document</span>\n",
    "<li>When a new document comes in\n",
    "<li>See which topic(s) it matches\n",
    "<li>We'll grab the first story on politico (if this works!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.politico.com\"\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text)\n",
    "links = soup.find_all('a',class_=\"js-tealium-tracking\")\n",
    "legit_links = list()\n",
    "for link in links:\n",
    "    if \"https://www.politico.com/news\" in link.get('href') and \"2023\" in link.get('href'):\n",
    "        legit_links.append(link)\n",
    "new_url = legit_links[0].get('href')\n",
    "new_soup = BeautifulSoup(requests.get(new_url).text)\n",
    "newdoc = ''\n",
    "for p in new_soup.find_all('p',class_=\"story-text__paragraph\"):\n",
    "    newdoc += p.get_text()\n",
    "newdoc    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Clean and set up the text\n",
    "<li>Create the word frequencies for this document using the original dictionary\n",
    "<li>You cannot add to the vocabulary when a new document arrives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slate_dictionary.doc2bow(newdoc.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "corpus_new = slate_dictionary.doc2bow(newdoc.lower().split())\n",
    "sorted(lda.get_document_topics(corpus_new,minimum_probability=0,per_word_topics=False),key=lambda x: x[1],reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.print_topic(topicno=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:30px\">Interpreting topics</span>\n",
    "<p></p>\n",
    "<li>What is the meaning of a topic? (what do the bunch of words and probabilities mean?)\n",
    "<li>How prevalent is the topic? (the relative weight of a topic across a corpus)\n",
    "<li>How do the topics relate to each other (are there overlaps? do they talk about the same essential thing?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Interpretation through word clouds</span>\n",
    "<p></p>\n",
    "<li>to better understand the topic we can draw wordclouds weighted by the weight of the terms in the topic</li>\n",
    "<li>the function, draw_word_clouds, draws the word clouds for all topics</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_topic returns the word, prob pairs for a topic\n",
    "for word,prob in lda.show_topic(1,topn=20):\n",
    "    print(word,prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_word_clouds(lda,num_topics,max_topic_words=30):\n",
    "    from wordcloud import WordCloud, STOPWORDS\n",
    "    import matplotlib.pyplot as plt\n",
    "    from random import shuffle\n",
    "    import math\n",
    "\n",
    "    COL_NUM = 2\n",
    "    ROW_NUM = math.ceil(num_topics/2)\n",
    "    \n",
    "    fig, axes = plt.subplots(ROW_NUM, COL_NUM, figsize=(12,12))\n",
    "\n",
    "    for i in range(0,num_topics):\n",
    "        word_list=[]\n",
    "        prob_total = 0\n",
    "        for word,prob in lda.show_topic(i,topn=max_topic_words):\n",
    "            prob_total +=prob\n",
    "        for word,prob in lda.show_topic(i,topn=max_topic_words):\n",
    "            if word in STOPWORDS:\n",
    "                continue\n",
    "            freq = int(prob/prob_total*1000)\n",
    "            alist=[word]\n",
    "            word_list.extend(alist*freq)\n",
    "            shuffle(word_list)\n",
    "            text = ' '.join(word_list)\n",
    "            wordcloud = WordCloud(stopwords=STOPWORDS,background_color='white',max_words=20).generate(text)\n",
    "            ax = axes[i//2, i%2] \n",
    "            ax.set_title(\"topic \" + str(i))\n",
    "            ax.imshow(wordcloud)\n",
    "            ax.axis('off')\n",
    "    plt.show()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_word_clouds(lda,5,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">pyLDAvis (python LDA visualizer)</span>\n",
    "<p></p>\n",
    "<li>A package for visualizing the results of an LDA</li>\n",
    "<li>install pyLDAvis</li>\n",
    "<li>Make sure you have the latest version of scipy!</li>\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "lda_display = pyLDAvis.gensim_models.prepare(lda, slate_topic_corpus, slate_dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Left pane</span>\n",
    "<p></p>\n",
    "<li>projects topics across a 2-D space (inter-topic distances/overlaps)\n",
    "<li>the area of each circle shows the relative prevalence of each topic across the corpus\n",
    "<li>the distance between the circles gives an estimate of how separated the topics are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:x-large\">Right pane</span>\n",
    "<p></p>\n",
    "<li>helps interpret terms that define a topic</li>\n",
    "<li><span style=\"color:green\">relevance</span>: tradesoff term frequency with inverse topic frequency. A term that is frequent in one topic but infrequent in other topics is more relevant for interpreting a topic</li>\n",
    "<li><span style=\"color:green\">lambda</span>: a factor used to compute relevance</li>\n",
    "<li>if lambda = 1, then the words will be ordered by the weights calculated by the lda</li>\n",
    "<li>if lambda = 0, then the words will be ordered by their relative rarity (i.e., the ratio of the weight calculated by the lda and the weight across all topics)</li>\n",
    "<li>barchart for each topic:\n",
    "<ol>\n",
    "<li>blue bars: frequency of each term in the corpus.\n",
    "<li>red bars: frequency within the topic. If lambda = 1, this is p(w,t) (e.g. P(dog|t2). Lambda adjusts the relevance using the overall frequency of the word.\n",
    "</ol>\n",
    "\n",
    "<li>some studies show that lambda =0.6 is a good tradeoff for well structured domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:green;font-size:30px\">Document Similarity</span>\n",
    "<p></p>\n",
    "<li>Given a corpus of documents, when a new document arrives, find the document that is the most similar</li>\n",
    "<li>the lda model helps find the most similar document</li>\n",
    "<li>this is useful because if you know what the content of the similar document is, you can estimate the topic(s) in the new document</li>\n",
    "<li>Construct a similarity matrix using the lda model (size = nxn where n is the number of documents)</li>\n",
    "<li>find the topic distribution for the new document</li>\n",
    "<li>match that distribution against the similarity matrix to find the closest matches</li>\n",
    "<li>lda uses <a href=\"https://www.machinelearningplus.com/nlp/cosine-similarity/\">cosine similarity </a></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "index = MatrixSimilarity(lda[slate_topic_corpus])\n",
    "new_topic_dist = lda[corpus_new]\n",
    "\n",
    "sims = index[new_topic_dist]\n",
    "print(sims)\n",
    "print(\"Most similar document\",np.argmax(sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size:40px\">Text summarization</span>\n",
    "<p></p>\n",
    "<li>Text summarization is useful because you can generate a short summary of a large piece of text automatically</li>\n",
    "<li>Analysts can read the short summary and decide whether to read the actual document or not</li>\n",
    "<li>Two types of text summarization</li>\n",
    "<ul>\n",
    "    <li><span style=\"color:blue\">extractive:</span> The algorithm selects \"important\" sentences and reports these sentences as the summary </li>\n",
    "    <li><span style=\"color:blue\">abstractive:</span> build an \"intelligent\" abstract of the document </li> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:xx-large\">Extractive summaries</span>\n",
    "<p></p>\n",
    "<li>The package \"sumy\" provides many summarization algorithms</li>\n",
    "<li><b>LexRank</b> identifies sentences that are the most similar to other sentences in the document. These are selected for the summary</li>\n",
    "<li><b>LatentSemanticAnalysis</b> applies SVD (singular value decomposition) to the word frequency vector and picks the sentences with the highest loading</li>\n",
    "<li><b>Luhn</b> looks at both the word frequency vector (term frequencies) as well as the <span style=\"color:blue\">inverse document frequency (idf)</span>. idf is the inverse of the number of documents that contain a word (words that occur in more documents are discounted)</li>\n",
    "<li>We'll walk through LexRank</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sumy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "my_parser = PlaintextParser.from_string(newdoc,Tokenizer('english'))\n",
    "lex_rank_summarizer = LexRankSummarizer()\n",
    "lexrank_summary = lex_rank_summarizer(my_parser.document,sentences_count=3)\n",
    "for sentence in lexrank_summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:xx-large\">Abstractive summaries</span>\n",
    "<p></p>\n",
    "<li>The idea with abstractive summaries is to create new sentences that summarize the article</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:xx-large\">Encoder-decoder models</span>\n",
    "<p></p>\n",
    "<li><b>Encoder</b> encodes the input into an intermediate form</li>\n",
    "<li><b>Decoder</b> decodes the intermediate form into the output</li>\n",
    "<li><span style=\"color:blue\">language translation</span> from English to Italian</li>\n",
    "<ul>\n",
    "    <li>since English and Italian have different grammers, an English sentence needs to be appropriately encoded for ease of translation into Italian</li>\n",
    "    <li>And the italian decoder has to be able to convert the encoded form of the english sentence into Italian</li>\n",
    "    <li>often, the encoding or the decoding is done using a neural network (see <a href=\"https://en.wikipedia.org/wiki/Recurrent_neural_network\">Recurrent neural networks</a>) but that's beyond the scope of our course</li>\n",
    "</ul>\n",
    "<li>Examples of abstractive summaries using encoder-decoder models, all with pretrained English encoders:</li>\n",
    "<ul>\n",
    "    <li><a href=\"https://huggingface.co/transformers/model_doc/t5.html\">T5 transformers</a></li>\n",
    "    <li><a href=\"https://huggingface.co/transformers/model_doc/bart.html\">BART transformers</a></li>\n",
    "    <li><a href=\"https://huggingface.co/transformers/model_doc/gpt.html\">GPT transformers</a></li>\n",
    "</ul>\n",
    "<p></p>\n",
    "<img src=\"encoder decoder.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:xx-large\">T5 summarizers</span>\n",
    "<p></p>\n",
    "<li>builds a summary by constructing its own text</li>\n",
    "<li>to activate the summary module, add \"summarize:\" to the front of the document</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,transformers,sentencepiece\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)\n",
    "print(sentencepiece.__version__)\n",
    "#1.13.0\n",
    "#4.24.0\n",
    "#0.1.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
    "\n",
    "#Use the pretrained encoder (you can also train your own encoder but ....)\n",
    "my_model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "\n",
    "#Create a tokenizer (i.e., a converter from text into tokens)\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"summarize:\" + newdoc\n",
    "#The encoder setup\n",
    "input_ids=tokenizer.encode(text, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the encoding. We're asking it to generate a minimum and maximum tokens\n",
    "#The summary is represented as a list of token ids\n",
    "summary = my_model.generate(input_ids,min_length=100,max_length=160000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the decoding. In this case, decoding is straight forward\n",
    "t5_summary = tokenizer.decode(summary[0],skip_special_tokens=True)\n",
    "print(t5_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdoc = \"\"\"\n",
    "Proud and prosperous Sweden, with its famously generous welfare system and abundance of green energy, \n",
    "should – in theory – be better equipped than most European countries to bear a battering from the continent’s \n",
    "cost of living crisis.\n",
    "\n",
    "In terms of GDP per capita, it is the EU’s fifth-richest member state. Natural gas accounts for only 2% of its \n",
    "energy, insulating it from the worst economic ravages of Russia’s war on Ukraine. Poverty is far below the European average.\n",
    "\n",
    "But fast-rising electricity bills and surging food price inflation are taking their toll here as elsewhere. \n",
    "“Sweden also has a poverty problem,” said Johan Rindevall. “We may not talk about it much, but it’s there – and it’s \n",
    "absolutely got worse this year.”\n",
    "\n",
    "Rindevall is well-placed to know. The 39-year-old former tech industry worker runs Matmissionen, or Food Mission, a \n",
    "unique chain of social supermarkets in Sweden that has expanded rapidly since January, more than doubling its customer \n",
    "numbers as it offers means-tested members the chance to shop for food for less.\n",
    "\n",
    "Matmissionen’s eight stores – five in Stockholm, three of which opened this year, two in Gothenberg and one in \n",
    "Malmö – sell food donated by producers and retailers that is at risk of being wasted, usually because it has cosmetic \n",
    "blemishes, damaged packaging or a short sell-by date.\n",
    "\n",
    "The organisation’s aim is threefold: to limit food waste, train new workers – about 70% of staff are on various job \n",
    "market insertion programmes, and 40% go on to find full-time work – and, above all, to sell food at very low prices to \n",
    "people who need it. Revenue from the stores also helps subsidise a separate foodbank operation with some donations \n",
    "distributed to NGOs working with those in the most extreme need, mostly the homeless.\n",
    "\n",
    "Rindevall says Matmissionen works on the principle of sticking as close to a familiar shopping experience for its \n",
    "customers as it can. “Our focus groups show there’s a real stigma around food handouts. So we decided to let them buy what \n",
    "they want, albeit at a very steep discount … It’s just more empowering that way,” he says. “People want things to be as normal as possible.”\n",
    "\n",
    "In fact anyone can shop at Matmissionen – but only registered members, who must book a slot to shop, get the lowest prices. \n",
    "Membership is open to those with a monthly income of less than 11,200 kronor (roughly £880) in pay or benefits. \n",
    "Membership prices are rock-bottom: five kronor (40p) for a loaf of bread, six for a kilo of bananas and 33 for 500g of minced beef.\n",
    "\n",
    "It is an offer that is increasingly needed. Sweden’s welfare system has been steadily cut back in recent years, widening \n",
    "the gap between rich and poor and leaving more and more people vulnerable to inflation that has averaged about 8% this autumn.\n",
    "\n",
    "Household incomes have also been hit by electricity bills that have in some cases doubled. More than 75% of Sweden’s electricity \n",
    "comes from hydropower, nuclear and wind, but it has not escaped the continent-wide energy price impacts of the war in Ukraine.\n",
    "\n",
    "Petrol and food prices have soared too. The cost of butter is up by about 25% this year, meat by 24% and cheese by about 22%, \n",
    "according to consumer price comparison sites.\n",
    "\n",
    "In practice, says Rindevall, 90-95% of purchases are by members, who can buy up to 300 kronor of food a week at the \n",
    "membership price – never more than 30% of the price in a discount supermarket – and as much as they want on top at a \n",
    "higher price. Few members are going hungry, but many are unable to afford a balanced diet: lots of carbs, little protein, few vegetables.\n",
    "\n",
    "He says Matmissionen’s membership climbed from 7,200 in January to more than 14,700 by the end of October. The biggest group \n",
    "of newcomers, about 40% , are families with children, both single parents and couples. “Inflation at these rates mean we’re \n",
    "seeing many, many more people than ever before. Some have started coming in saying they don’t qualify as members, but can’t afford \n",
    "to buy the food they need anywhere else,” he says.\n",
    "\n",
    "According to Sweden’s Central Statistics Office, during the country’s last major inflationary period in the early 1990s, about 7% \n",
    "of the population were in relative poverty – defined as living on 60% of less of the median income. This year, that percentage is \n",
    "estimated to be above 14%.\n",
    "\n",
    "Matmissionen is drawing up expansion plans for new stores across the country. It recently reached agreements with both the Swedish \n",
    "food retailers’ association and the national federation of food producers and distributors, guaranteeing the support of almost the \n",
    "entire food sector.\n",
    "\n",
    "“Sweden may still have a good safety net, but it maybe isn’t reactive enough to sudden, big cost of living changes,” says Rindevall.\n",
    "\n",
    "“The only positive thing in all this is that now so many people are talking about impossible food prices that there’s no longer \n",
    "the same stigma in not being able to afford to feed your family. It’s no longer a taboo.”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Compare with extractive summary</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "my_parser = PlaintextParser.from_string(newdoc,Tokenizer('english'))\n",
    "lex_rank_summarizer = LexRankSummarizer()\n",
    "lexrank_summary = lex_rank_summarizer(my_parser.document,sentences_count=3)\n",
    "for sentence in lexrank_summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
